import numpy as np
import re

# å®šç¾©è²æ¯ï¼ˆinitialsï¼‰èˆ‡éŸ»æ¯ï¼ˆfinalsï¼‰
consonants = ['p', 'ph', 'm', 'b', 't', 'th', 'n', 'l', 'k', 'kh', 'ng', 'g', 'ts', 'tsh', 's', 'j', 'h']
vowels = sorted([
    'm', 'mh', 'ng', 'ngh', 
    'a', 'ah', 'ann', 'annh', 'am', 'ap', 'an', 'at', 'ang', 'ak',
    'e', 'eh', 'enn', 'ennh', 'eng',
    'ee', 'eeh',
    'er', 'erh', 'erm',
    'i', 'ih', 'inn', 'innh', 'im', 'ip', 'in', 'it', 'ing', 'ik',
    'ir', 'irm', 'irp', 'irn', 'irt', 'irng', 'irk', 
    'o', 'oh', 'or', 'orh', 
    'oo', 'ooh', 'onn', 'onnh', 'om', 'op', 'ong', 'ok',
    'u', 'uh', 'un', 'ut', 
    'ai', 'aih', 'ainn', 'ainnh',
    'au', 'auh', 'aunn', 'aunnh',
    'ia', 'iah', 'iann', 'iannh', 'iam', 'iap', 'ian', 'iat', 'iang', 'iak', 
    'io', 'ioh', 'ior', 'iorh', 'ioo', 'ionn', 'iong', 'iok', 
    'iu', 'iuh', 'iunn', 'iunnh', 
    'iau', 'iauh', 'iaunn',
    'ua', 'uah', 'uann', 'uan', 'uat', 'uang', 
    'ue', 'ueh',
    'ere', 'ereh',
    'uee', 
    'ui', 'uih', 'uinn',
    'irinn',
    'uai', 'uaih', 'uainn', 'uainnh'
], key=len, reverse=True)  # ç¢ºä¿é•·éŸ»æ¯å„ªå…ˆåŒ¹é…

def preprocess_text(text):
    """ å»é™¤è²èª¿æ•¸å­—å’Œ '-'ï¼Œä¸¦åŸ·è¡Œ initial-final æ‹†åˆ† """
    text = text.lower()  # è‹±æ–‡æ­£è¦åŒ–ç‚ºå°å¯«
    text = re.sub(r'[-\d]', ' ', text)  # å»æ‰èª¿è™Ÿ
    words = text.split()
    split_words = []

    for word in words:
        initial, final = split_pinyin(word)
        if initial and final:
            split_words.append(initial + " " + final)
        elif final:
            split_words.append("- " + final)
    
    return " ".join(split_words)

def split_pinyin(word):
    """ æ‹†åˆ†è²æ¯ï¼ˆinitialï¼‰èˆ‡éŸ»æ¯ï¼ˆfinalï¼‰ """
    for vowel in vowels:
        if word.endswith(vowel):
            initial = word[:-len(vowel)]
            if initial in consonants:
                return initial, vowel
            else:
                return ' ', word  # æ²’æœ‰è²æ¯æ™‚ç•¶ä½œéŸ»æ¯
    return None, None

def normalize_spacing(text):
    """ ç¢ºä¿æ–‡æœ¬ç©ºæ ¼ä¸€è‡´ """
    return " ".join(text.split())

def calculate_wer_details(reference, hypothesis):
    """
    è¨ˆç®— WERï¼ˆWord Error Rateï¼‰ï¼Œä¸¦æ¨™è¨˜éŒ¯èª¤é¡å‹ï¼ˆæ’å…¥ Iã€åˆªé™¤ Dã€æ›¿æ› Sï¼‰
    """
    reference = reference.lower().strip()
    hypothesis = hypothesis.lower().strip()

    ref_words = reference.split()
    hyp_words = hypothesis.split()
    len_ref = len(ref_words)
    len_hyp = len(hyp_words)

    # å»ºç«‹ DP è¡¨
    dp = np.zeros((len_ref + 1, len_hyp + 1), dtype=np.int32)
    for i in range(len_ref + 1):
        dp[i][0] = i
    for j in range(len_hyp + 1):
        dp[0][j] = j

    for i in range(1, len_ref + 1):
        for j in range(1, len_hyp + 1):
            if ref_words[i - 1] == hyp_words[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = min(dp[i - 1][j] + 1,  # åˆªé™¤
                               dp[i][j - 1] + 1,  # æ’å…¥
                               dp[i - 1][j - 1] + 1)  # æ›¿æ›

    wer_value = dp[len_ref][len_hyp] / max(1, len_ref)

    return wer_value

def calculate_wer_from_files(ref_file, hyp_file, output_file):
    with open(ref_file, 'r', encoding='utf-8') as ref_f, open(hyp_file, 'r', encoding='utf-8') as hyp_f:
        ref_lines = ref_f.readlines()
        hyp_lines = hyp_f.readlines()

    output = []
    error_log = []

    for idx, (ref_line, hyp_line) in enumerate(zip(ref_lines, hyp_lines), start=1):
        try:
            ref_line = ref_line.strip()
            hyp_line = hyp_line.strip()
            if " " not in ref_line or " " not in hyp_line:
                raise ValueError(f"Line {idx} is malformed: '{ref_line}' | '{hyp_line}'")

            ref_id, ref_text = ref_line.split(' ', 1)
            hyp_id, hyp_text = hyp_line.split(' ', 1)

            ref_text = normalize_spacing(preprocess_text(ref_text))
            hyp_text = normalize_spacing(preprocess_text(hyp_text))

            wer = calculate_wer_details(ref_text, hyp_text)

            output.append(f"File: {ref_id}")
            output.append(f"Reference: {ref_text}")
            output.append(f"Hypothesis: {hyp_text}")
            output.append(f"PER: {wer:.4f}")
            output.append("=" * 50)

            print(f"âœ… è¨ˆç®— {ref_id}: WER={wer:.4f}")

        except ValueError as e:
            error_log.append(f"âŒ Line {idx} Error: {e}")
            print(f"âŒ Line {idx} Error: {e}")
            continue  # è·³éé€™ä¸€è¡Œ

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(output))

    if error_log:
        with open("error_log.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(error_log))
        print("âš ï¸ éƒ¨åˆ†è¡Œæœ‰å•é¡Œï¼Œå·²è¨˜éŒ„åˆ° error_log.txt")

    print(f"ğŸ“„ WER çµæœå·²ä¿å­˜è‡³ {output_file}")

# ä½¿ç”¨æ–¹æ³•
teacher_model_transcriptions = "teacher.txt"
student_model_transcriptions = "student.txt"
output_file = "Pseudo_PER.txt"

calculate_wer_from_files(teacher_model_transcriptions, student_model_transcriptions, output_file)
